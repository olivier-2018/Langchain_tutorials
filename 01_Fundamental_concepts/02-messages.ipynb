{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ad1dea",
   "metadata": {},
   "source": [
    "# Messages\n",
    "source: https://python.langchain.com/docs/how_to/#messages\n",
    "\n",
    "Langchain **Messages** are the input and output of chat models.  \n",
    "They have some content and a role, which describes the source of the message.\n",
    "\n",
    "Types of usage:\n",
    "- trim messages\n",
    "- filter messages\n",
    "- merge consecutive messages of the same type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c0179",
   "metadata": {},
   "source": [
    "## Trim messages\n",
    "\n",
    "All models have finite context windows, meaning there's a limit to how many tokens they can take as input. If you have very long messages or a chain/agent that accumulates a long message history, you'll need to manage the length of the messages you're passing in to the model.\n",
    "\n",
    "**trim_message** can be used to reduce the size of a chat history to a specified token count or specified message count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79294d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'--- BEFORE TRIM: \\n<function get_msgs at 0x7101dfdc6520>'\n",
      "\n",
      "\n",
      "--- ATER TRIM:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage, trim_messages\n",
    "\n",
    "def get_msgs():\n",
    "    msg = [\n",
    "        SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "        HumanMessage(\"i wonder why it's called langchain\"),\n",
    "        AIMessage('Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'),\n",
    "        HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "        AIMessage( \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
    "        HumanMessage(\"what do you call a speechless parrot\"),\n",
    "    ]\n",
    "    return msg\n",
    "\n",
    "messages = get_msgs()\n",
    "\n",
    "pprint(f'--- BEFORE TRIM: \\n{get_msgs}')\n",
    "print('\\n\\n--- ATER TRIM:')\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    # Keep the last <= n_count tokens of the messages.\n",
    "    strategy=\"last\",\n",
    "    \n",
    "    # Remember to adjust based on your model or else pass a custom token_counter\n",
    "    token_counter=count_tokens_approximately,\n",
    "    \n",
    "    # Most chat models expect that chat history starts with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a SystemMessage followed by a HumanMessage\n",
    "    \n",
    "    # Remember to adjust based on the desired conversation length\n",
    "    max_tokens=45,\n",
    "    \n",
    "    # Most chat models expect that chat history starts with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a SystemMessage followed by a HumanMessage\n",
    "    start_on=\"human\",\n",
    "    \n",
    "    # Most chat models expect that chat history ends with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a ToolMessage\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    \n",
    "    # Usually, we want to keep the SystemMessage if it's present in the original history.\n",
    "    # The SystemMessage has special instructions for the model.\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d2c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fe0e8a",
   "metadata": {},
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b189c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alcor/13_LLMs+AI_agents/12_Langchain/Langchain_tutorials/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a speechless parrot?  \\nA *parrot* that’s “*mutt*‑ed” into silence—because even birds can have a *mute*‑ary!', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-09-08T09:56:46.245168838Z', 'done': True, 'done_reason': 'stop', 'total_duration': 61396747112, 'load_duration': 9643177903, 'prompt_eval_count': 96, 'prompt_eval_duration': 383365400, 'eval_count': 7115, 'eval_duration': 51295714130, 'model_name': 'gpt-oss:20b'}, id='run--83ba9e80-b073-4f5b-9aca-81cdd63db5ae-0', usage_metadata={'input_tokens': 96, 'output_tokens': 7115, 'total_tokens': 7211})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage, trim_messages\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"gpt-oss:20b\", temperature=0.)\n",
    "\n",
    "\n",
    "# Notice we don't pass in messages. This creates\n",
    "# a RunnableLambda that takes messages as input\n",
    "trimmer = trim_messages(\n",
    "    token_counter=llm,\n",
    "    # Keep the last <= n_count tokens of the messages.\n",
    "    strategy=\"last\",\n",
    "    # When token_counter=len, each message\n",
    "    # will be counted as a single token.\n",
    "    # Remember to adjust for your use case\n",
    "    max_tokens=45,\n",
    "    # Most chat models expect that chat history starts with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a SystemMessage followed by a HumanMessage\n",
    "    start_on=\"human\",\n",
    "    # Most chat models expect that chat history ends with either:\n",
    "    # (1) a HumanMessage or\n",
    "    # (2) a ToolMessage\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    # Usually, we want to keep the SystemMessage\n",
    "    # if it's present in the original history.\n",
    "    # The SystemMessage has special instructions for the model.\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | llm\n",
    "\n",
    "messages = get_msgs()\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277a284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
