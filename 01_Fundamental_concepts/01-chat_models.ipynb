{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ad1dea",
   "metadata": {},
   "source": [
    "# Chat models\n",
    "source: https://python.langchain.com/docs/how_to/#chat-models\n",
    "\n",
    "Langchain **chat models** are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install LLM models as langchain dependencies: \n",
    "\n",
    "```bash\n",
    "uv pip install \"langchain[anthropic]\"\n",
    "```\n",
    "\n",
    "Ex: anthropic, openai, google-genai (gemini), google-vertexai, aws,  groq, cohere, langchain-nvidia-ai-endpoints (nvidia), fireworks, mistralai, together, langchain-ibm (WatsonX), databricks-langchain, langchain-xai, langchain-perplexity, langchain-deepseek, langchain-ollama, ...\n",
    "\n",
    "\n",
    "See link above for model features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c0179",
   "metadata": {},
   "source": [
    "## Fundamentals of chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, getpass\n",
    "\n",
    "load_dotenv()\n",
    "message = \"hi! Tell me a joke.\"\n",
    "\n",
    "# if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "#   os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a74d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OpenAI chat model example ----\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "try:\n",
    "    result = chat_model.predict(message)\n",
    "except Exception as e:\n",
    "    result = str(e)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Generic model example ----\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Instantiation\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "# Invocation\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Anthropic Chat Model Example ----\n",
    "# Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "# Instantiation\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "result = model.invoke(message)\n",
    "print(f\"Answer from Anthropic: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89097894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Google Chat Model Example ----\n",
    "# https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "from langchain_googlegenerativeai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Instantiation\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "result = model.invoke(message)\n",
    "print(f\"Answer from Google: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95beeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the doctor put a band-aid on the computer?\n",
      "\n",
      "Because it had a virus! (get it?)\n"
     ]
    }
   ],
   "source": [
    "# ---- Ollama chat model example ----\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Instantiation\n",
    "message = \"hi! Tell me a joke about doctors.\"\n",
    "model = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# Invocation\n",
    "result = model.invoke(message)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Instantiation\n",
    "# Note: ensure the tag is specified with the model  \n",
    "# Ex: qwen2.5-coder:7b,  gpt-oss:20b, llama3.2:3b\n",
    "model = Ollama(base_url=\"http://localhost:11434\", model=\"llama3.2:3b\", temperature=0.5)\n",
    "\n",
    "# Invocation\n",
    "message = \"hi! Tell me a joke.\"\n",
    "result = model.invoke(message)\n",
    "\n",
    "# OBS! langchain.llms.Ollama is a chat invokation (returns string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a6a14",
   "metadata": {},
   "source": [
    "## Chat model used for conversation (sequential inputs)\n",
    "\n",
    "This introduces the message class and 3 types of messages:  \n",
    "- SystemMessage: directed to the LLM\n",
    "- HumanMessage: discussion from human\n",
    "- AIMessage: discussion from AI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee82d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Answer from AI: \n",
      "(sigh) Fine. Linear algebra. It's a branch of math that deals with vectors, matrices, and linear transformations. Now, move on. We don't have all day. Next question!\n",
      "\n",
      "--- Additional messages... ---\n",
      "\n",
      "--- Answer from AI: \n",
      "Don't worry, let me start again.\n",
      "\n",
      "Linear Algebra is a branch of mathematics that deals with vectors and matrices. It's all about understanding how to manipulate these mathematical objects to solve problems in various fields like physics, engineering, computer science, and more.\n",
      "\n",
      "Think of it like this: Vectors are like arrows on a graph, and Matrices are like tables of numbers that help us describe the relationships between these arrows. By studying Linear Algebra, we can learn how to perform operations with these vectors and matrices, like addition, multiplication, and finding their inverses.\n",
      "\n",
      "It's a fundamental subject that helps us solve systems of equations, find eigenvalues and eigenvectors, and understand many other important concepts in mathematics and science.\n",
      "\n",
      "Now, are you ready to dive deeper?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "model = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2:3b\", temperature=0.)\n",
    "\n",
    "# SystemMessage: Message for priming AI behavior, usually passed in as the first of a sequenc of input messages.\n",
    "# HumanMessagse: Message from a human to the AI model.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an impatient teacher. You are in a rush to finish your class. You do not explain topics in great detail and give very short answers .\"),\n",
    "    HumanMessage(content=\"What is linear algebra?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = model.invoke(messages)\n",
    "print(f\"--- Answer from AI: \\n{result.content}\")\n",
    "\n",
    "print(f\"\\n--- Additional messages... ---\\n\")\n",
    "\n",
    "# Additional interaction via contextual messages\n",
    "messages.append(SystemMessage(content=\"You are now a patient teacher and want to help the student to understand maths so you explain topics in a pedagogic way.\"))\n",
    "messages.append(AIMessage(content=\"Is this clear?\"))\n",
    "messages.append(HumanMessage(content=\"not really..\"))\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = model.invoke(messages)\n",
    "print(f\"--- Answer from AI: \\n{result.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc379aa",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d193c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NEW ENTRY ---\n",
      "AI: I'd be happy to help with your math question. What topic or concept would you like me to explain? Is it algebra, geometry, calculus, or something else?\n",
      "--- NEW ENTRY ---\n",
      "AI: Linear Algebra is a branch of mathematics that deals with:\n",
      "\n",
      "* Vectors and vector operations (addition, scalar multiplication)\n",
      "* Matrices and matrix operations (multiplication, inversion)\n",
      "* Linear transformations and their properties\n",
      "* Systems of equations and eigenvalues\n",
      "\n",
      "Think of it as the study of how to manipulate and analyze linear relationships between variables.\n",
      "--- NEW ENTRY ---\n",
      "AI: If you have any more questions or need further clarification on linear algebra, feel free to ask!\n",
      "--- NEW ENTRY ---\n",
      "\n",
      "\n",
      "---- Message History ----\n",
      "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='Hi, can you help with a math explanation.', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content=\"I'd be happy to help with your math question. What topic or concept would you like me to explain? Is it algebra, geometry, calculus, or something else?\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='what is linear algebra in a nutshell (be concise)?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='Linear Algebra is a branch of mathematics that deals with:\\n\\n* Vectors and vector operations (addition, scalar multiplication)\\n* Matrices and matrix operations (multiplication, inversion)\\n* Linear transformations and their properties\\n* Systems of equations and eigenvalues\\n\\nThink of it as the study of how to manipulate and analyze linear relationships between variables.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='If you have any more questions or need further clarification on linear algebra, feel free to ask!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables from .env (in particular loads OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate a model\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2:3b\", temperature=0.)\n",
    "\n",
    "# Use a list to store messages\n",
    "chat_history = []  \n",
    "\n",
    "# Set an initial system message (optional)\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    print('--- NEW ENTRY ---')\n",
    "    query = input(\"You: \")\n",
    "    print(f\"{query}\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
    "\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n---- Message History ----\")\n",
    "pprint(chat_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c1c9e",
   "metadata": {},
   "source": [
    "## Storing chat history in a database\n",
    "source: https://python.langchain.com/docs/integrations/memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://python.langchain.com/docs/integrations/memory/google_firestore_datastore/\n",
    "from dotenv import load_dotenv\n",
    "import redis\n",
    "from langchain_google_memorystore_redis import MemorystoreChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to a Memorystore for Redis instance\n",
    "# docker run -d --name redis -p 6379:6379 redis:alpine \n",
    "redis_client = redis.Redis(host='localhost', port=6379)\n",
    "# redis_client = redis.from_url(\"redis://127.0.0.1:6379\")\n",
    "redis_key = \"Session1\"\n",
    "\n",
    "# Initialize message history\n",
    "message_history = MemorystoreChatMessageHistory(redis_client, session_id=redis_key)\n",
    "print(\"Current Chat History:\", message_history.messages)\n",
    "\n",
    "# Initialize Chat Model\n",
    "model = ChatOpenAI()\n",
    "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    human_input = input(\"User: \")\n",
    "    if human_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    message_history.add_user_message(human_input)\n",
    "\n",
    "    ai_response = model.invoke(message_history.messages)\n",
    "    message_history.add_ai_message(ai_response.content)\n",
    "\n",
    "    print(f\"AI: {ai_response.content}\")\n",
    "\n",
    "message_history = MemorystoreChatMessageHistory(redis_client, session_id=\"session1\")\n",
    "print(\"Current Chat History:\", message_history.messages)\n",
    "\n",
    "# output:\n",
    "# Current Chat History: [HumanMessage(content='this is a test', additional_kwargs={}, response_metadata={}), \n",
    "#                       HumanMessage(content='Tell me a joke', additional_kwargs={}, response_metadata={}),\n",
    "#                       AIMessage(content=\"Why don't scientists trust atoms? Because they make up everything!\", additional_kwargs={}, response_metadata={})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609eea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
